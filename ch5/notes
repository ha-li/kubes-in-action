topics in ch5
-the main theme of ch5 is services
-topics include:
 * exposing groups of pods as single ip using Service resource
     - creating services
     - session affinity services
     - multi-port services
 * services discovery in the cluster
     - external service resources
     - endpoint resources
     - external name alias
 * exposing services to external clients
     - using service type - nodeport
     - using service type - loadbalancer
     - using ingress resource
 * finding/connecting to external services
 * controlling pod readiness as part of a service
 * trouble shooting services


Most applications are meant to respond to external requests
 eg: http requests coming from outside pods or 
     events from external infrastructure (jms)


In order to be able to consume other services an application must
be able to find those pods
 - in the old non-kubernetes world, a sys admin would be able to 
   configure each client app by specifying the exact ip address 
   or dns of the host
 - in kubernetes this is not possible because 
   * pods are constantly moving around (ephemeral)
   * horizontal scaling means multiple pods provide the same service
   * ips are assigned after the pod has been schedule to a node so
     clients will not know the ip address of the server up front

- to solve for this, kubernetes introduces the concept of a 
  Service resource

Kubernetes Service
==================
A Kubernetes Service is a resource you create that provides a single
and consistent point of entry to a group of pods that do the same thing
- each service ip and port never changes while the service exists
- clients can connect to that ip and port to get access to the service
  provided by the pods backing that service resource
- clients don't know who the individual pods are and the count of those pods,
  allows the pods to die and be re-created, move as needed

eg
a concrete example of this is a cluster of front end (ui) pods needing to
connect to a back end database server
- external clients need to be able to connect to the ui pods without concern
  if there is only 1 or hundreds of instances of the front end
- the front end pods need to connect to the back end database, but since
  the database is running inside a pod, it may be moved around the cluster
  any time, so its ip address may change, you want to allow this, without
  having to reconfigure the front end pods each time the backend moves
- so it this case there will be two services, one load balancing the front end
  and one in front of the database

Creating Services
-----------------
-client connections to the Service are load balanced across the pods of a Service
-individual pods are set to a Service by the use of label selectors
-Service resources are created just like any other kubernete resources, via a file 
 manifest

 eg: kubia-svc.yml

apiVersion: v1
kind: Service
metadata:
   name: kubia-svc
spec:
   ports:
   - port: 80
     targetPort: 8080
   selector:
     app: kubia

-from the manifest: 
  * the kind is Service
  * the spec defines the port the service listens to client connections on (80)
  * the target port is the pod port the service will forward to (8080), this is
    what the pod application will handle requests on
  * the selector defines which pods will be part of this service
 so this Service will listen on port 80, and route each connection to port 8080
 of one of the pods matching app=kubia label selector 

Listing Services
----------------
to get the listing of services in you namespace

$ kubectl get svc
  NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
  kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   19d
  kubia        ClusterIP   10.99.161.109   <none>        80/TCP    1d

The cluster ip is what pods inside the cluster connect to, so if your client is
a part of this cluster and would like to use the service kubia, it will
use the ip 10.99.161.109, port 80

For pods that are external to the cluster, they need to use the external ip,
which is covered later on

But for the most part, services primary function is to provide a service 
internal to the cluster

Testing the Service
-------------------
several ways to test the service:
- create a client pod that will send a request to the service ip address
  and log the response of the service, you can then examine the pods log
- ssh into the kubernetes nodes and use the curl command
- execute the curl command inside an existing pod using the 'kubectl exec' 
  command
  
  eg of executing the curl command inside an existing pod:

  $ kubectl exec kubia-7nadol -- curl -s http://10.99.161.109
  You've hit kubia-aawezg

the kubectl exec command allows you to run a command inside a kubernetes 
pod remotely. As long as you have kubectl installed, you issue
kubectl exec with the pod you want to use, and then the command
you want to run inside the pod


   kubectl exec
   kubia-7nadol                    <-- the pod i'm using as the host
   --                              <-- separator
   curl -s http://10.99.161.109    <-- the command to run inside the host pod


this will send a curl command to the Service from above, and return the 
response of the Service back to you, via the host pod

the double dash (--) signals the end of the command options to kubectl, and
anything following the -- is the actual command to run 

kubectl exec is analogous to ssh into a pod, and then issuing the command 
inside the pod, but done remotely in a elegant fashion for the user


Service Affinity Configuration
------------------------------
In a normal service, the load balancer randomly selects a pod
each time.

A Service resource can be configured to be sticky with a simple 
configuration in the manifest
-sticky services will remember which pod was selected for each
 client, and remember reuse the same pod for that client

 apiVersion: v1
 kind: Service
 spec:
   sessionAffinity: ClientIP
 ...

-this configuration will make the service proxy all requests 
originating from the same client to the same pod.

kubernetes does not support stickiness through cookie-based
sessions because cookies are built on top of http
-kubernetes session affinity does not operate the the http level
-services deals with tcp/udp packets and do not care about the
 payload they carry


Multi-Port Services
-------------------
-a normal service exposes only 1 port, but you can configure 
 multi-port services, 
   ex if your pod application listens on both 80 and 443 
      you could create a service to listen to 8080 for http, 
      8443 for https and have the service forward to 
      80 and 443 on the pods

-creating multi-port service is done through configurating
 the manifest:

 apiVersion: v1
 kind: Service
 metadata: 
   name: kubia
 spec:
   ports:
     - name: http
       port: 80
       targetPort: 8080
     - name: https
       port: 443
       targetPort: 8443
     selector: 
       app: kubia

 this service exposes 2 ports, and applies the service to 
 any pods with app=kubia labels


Named Ports
-----------
When defining the ports on your pod, you can give them names

 apiVersion: v1
 kind: Pod
 spec:
   containers:
   - name: kubia
     ports:
     - name: http
       containerPort: 8080
     - name: https
       containerPort: 8443

Then in the Service manifest, you can refer to the target ports 
by their names:

 apiVersion: v1
 kind: Service
 spec:
   ports:
   - name: http
     port: 80
     targetPort: http
   - name: https:
     port: 443
     targetPort: https
   selector:
     app: kubia


Service Discovery
-----------------
Kubernetes provides ways for client pods to discover a service ip and port
using:
  environment variables
  dns

Env Variables
-------------
-when a pod is first started up, kubernetes will create environment variables
 of each existing service available at that time
   - as long as the service is up before the pod, the pod will have access
     to an env variable pointing to the ip and port of that service
   - processes on the pod can then inspect their env variables to find
     the services available

to see what environment variables are available inside a pod:
$ kubectl exec kubia-2jai3n env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-b2xcx
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBIA_SERVICE_PORT=80
KUBIA_PORT_80_TCP_PROTO=tcp
KUBIA_PORT_80_TCP_ADDR=10.99.161.109
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBIA_SERVICE_HOST=10.99.161.109
KUBIA_PORT=tcp://10.99.161.109:80
KUBIA_PORT_80_TCP=tcp://10.99.161.109:80
KUBIA_PORT_80_TCP_PORT=80
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT_443_TCP_PORT=443
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=7.10.1
YARN_VERSION=0.24.4
HOME=/root 


-from the list of environment variables you can see that the
 service has an env with it's name in all caps 
-compare this output with the results of 
$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   19d
kubia        ClusterIP   10.99.161.109   <none>        80/TCP    1d


So if you had a database service, you can name it backend-database,
then its env would be BACKEND_DATABASE_SERVICE_HOST and 
BACKEND_DATABASE_SERVICE_PORT


Service Discovery thru DNS
--------------------------
In the kube-system namespace, you can find the DNS service called kube-dns

-to switch namespace
 $ kubectl config set-context $(kubectl config current-context) --namespace kube-system

-to show the services
 $ kubectl get svc

   NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
   kube-dns               ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP   19d
   kubernetes-dashboard   NodePort    10.97.247.33   <none>        80:30000/TCP    19d

All the pods in the cluster are configured to use this dns
-kubernetes automatically modifies each containers /etc/resolv.conf file
-any dns query performed by a process on the pod will be handled by kube-dns
-kube-dns knows all the services running in the system
-each service gets a dns entry in the internal dns server
  client pods that know the name of the service can access it thru its 
  fully qualified domain name (fqdn), rather then thru env variables
   eg for a back end database, the fqdn may be:
       backend-database.default.svc.cluster.local

   the fqdn is broken up into parts: servicename.namespace.cluster domain suffix
      so service-name = backend-database
         namespace = default
         cluster-domain-suffix = svc.cluster.local
   
   dns only provides the host. the client must know the port number, which
   means you should keep to standard ports (80, 443, 1521 etc), if not
   the pod client can get the port number from the env variable

   you can also omit the cluster-domain-suffix and even the namespace when
   the client pod is in the same cluster and namespace as the service
    allowing you to refere to the service as: backend-database

(whether a pod uses the internal dns server or not is configurable thru the 
 'dnsPolicy' property of each pods spec)


To illustrate the ability to find services through the dns service, login one of hte
pods:
   $ kubectl exec -it kubia-b2xcx bash

then inside, run curl to hit the the kubia service (with and without the cluster,
   namespace)

   root@kubia-b2xcx:/# curl http://kubia.default.svc.cluster.local
   You've hit kubia-42na43

   root@kubia-b2xcx:/# curl http://kubia.default
   You've hit kubia-32ah34

   root@kubia-b2xcx:/# curl http://kubia
   You've hit kubia-23b345

And if you cat /etc/resolv.conf, you should see:
    root@kubia-b2xcx:/# cat /etc/resolv.conf
    nameserver 10.96.0.10
    search default.svc.cluster.local svc.cluster.local cluster.local
    ...

Pinging Services Does not Work
------------------------------
Curling a service works, but pinging it will not work.
The service ip is a virtual ip, and only works with the service port.
Ping is a different protocol, so attempting to ping it will not work.

Ping is ICMP based - ICM protocol does not have the concept of ports
Ping has no port. (ICMP - Internet Control Message Protocol) 

Ports belong to transport layer protocols like tcp and udp. 

You can use ping inside the pods to ping other services (like google.com)
but you cannot ping your services in the cluster unless the service
has opened the port/icmp 


Endpoint Resources
------------------
Services appear to be a load-balancer to pods, but actually Services are not directly
connected to the pod resources.
- another type of resource sits between the Service resource and the Pod resource
- endpoint resources

When you do a describe on a servce, you will see the endpoint resources:
 $ kubectl describe svc kubia
   Name:              kubia
   Namespace:         default
   Labels:            <none>
   Annotations:       <none>
   Selector:          app=kubia
   Type:              ClusterIP
   IP:                10.99.161.109
   Port:              <unset>  80/TCP
   TargetPort:        8080/TCP
   Endpoints:         172.17.0.5:8080,172.17.0.6:8080,172.17.0.7:8080
   Session Affinity:  None
   Events:            <none>

 In the listing, the endpoints are clear.


Enpoints reource is a list of IP addresses/ports exposing a service.
Like any other kubernetes resource, you can display its basic info

 $ kubectl get endpoints kubia
   NAME      ENDPOINTS                                         AGE
   kubia     172.17.0.5:8080,172.17.0.6:8080,172.17.0.7:8080   2d

So even though the pod selector is defined in the service spec, it
is not used directly, but rather, the selector is used to build
a list of the ip/port exposing the service, which get stored
in the endpoint resource. When a client resource connects to the
service, the service proxy will select one of the endpoint ip/port
values and redirect the incoming connections to that server.

By having the service decoupled from the endpoints in this way, 
you can reconfigure the services and update them manually.

Creating a Service Endpoint Manually
------------------------------------
You can create a service without a pod selector
-kubernetes will not create the Endpoints resource (because it doesn't
 know what to include in the service)
-you will have to create both a Service and Endpoints resource

Creating a Service
-to create a service manually and then assign the endpoints to 
 that service yourself, first create a Service resource, 
 without the pod selector

 eg external-service.yml

 apiVersion: v1
 kind: Service
 metadata:
   name: external-service
 spec:
    ports:
    - port: 80

-the endpoints service name should match the Service name

 eg external-service-endpoints.yml

 apiVersion: v1
 kind: Endpoints
 metadata
   name: external-service
 spec:
   - addresses:
     - ip: 11.11.11.11
     - ip: 22.22.22.22
     ports:
     - port: 80

-the endpoints service name matches the service name and
 contains the ip/port of the service
-once these are created, 
    the service can be used like any other service (fqdn or short name)
    pods created after these will have envirnoment variables for the service
    the ip address will be load balanced betweeen the endpoints

-later on, if you migrate these services to kubernetes, you just need to
 add a selector to the service, at which point, the endpoints become
 automatically managed

-if you have an existing Service resource with a selector, simply removing
 the selector tells kubernetes to stop updating the endpoints


Creating an Alias for an external service
-----------------------------------------

Say you have an external service you want to make use of like api.vendorco.com
and you want to create an alias for this service so that inside your
cluster, you can reference the external service using the alias. To do so:
 - create a Service resource with spec.type = ExternalName

 eg:
   apiVersion: v1
   kind: Service
   metadata:
     name: external-service
   spec:
     type: ExternalName
     # the fqdn
     externalName: api.vendorco.com
     ports: 
     - port: 80

Once this service is created, pods inside the cluster can refer to this service
by 'external-service' or 'external-service.default.svc.cluster.local' instead
of its fqdn api.vendorco.com

- the advantage of alias is that later on you can move the service without
  changing the alias, so that your client pods can continue to point to
  the alias, and you can change the underlying implementation simpley
  by changing the value of externalName or by changing the type back to ClientIP

- ExternalName service are implemented at the DNS level by a simple cname record



Exposing Services to External Clients
=====================================

There are 3 main ways to expose your own service to external clients
 * nodeport service
 * loadbalancer service
 * ingress resource

NodePort Service
----------------
-in a nodeport service, each node will open a designated port, and all incoming
 traffic on that port will be redirected to the service
-this means that the service will be accessible at its internal ip address/port
 as well as all nodes on that designated port

- to create a nodeport service: eg kubia-svc-nodeport.yml
  
  apiVersion: v1
  kind: Service
  metadata:
    name: kubia-nodeport
  spec:
    type: NodePort          <--- type
    ports:
    - port: 80              <--- port for internal cluster calls
      targetPort: 8080      <--- target port on the pod
      nodePort: 30123       <--- node port 
    selector:
      app: kubia

-if you don't specify the specific nodeport to use, kubernetes will select one randomly for you

-to see the basic infor of your service:
  $ kubectl get svc kubia-nodeport
    NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
    kubia-nodeport   NodePort   10.98.250.158   <nodes>       80:30123/TCP   2m

    - the type clearly shows NodePort
    - the external ip should show <nodes>
    - the port(s) should show both the internal and node port

    - when you see a NodePort, you should immediately realize that it the service is
      accessible on every node on that designated nodeport (30123)

- you should be able to get the IPs of all your nodes by jsonpath (simpilar to xpath) :
  $ kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
  
- on minikube, you can access NodePort services through the command:
  $ minikube service <service-name> -n <namespace> 
    eg minikube service kubia-nodeport
    ( this will open up the browser and navigate to:  192.168.99.101:30123)
     
     so now you can curl this ip:port
  $ curl 192.168.99.101:30123
      You've hit kubia-b2xcx

A good service to offer through NodePort would be IUS/AXS - except that nodeports services
expose the service to external world also.

NodePort types are a great way to expose services to the external world, however, if
the client only points to one node, if that node fails, your client won't be able
to acccess the service any more.

It is better to put a loadbalancer in front of the service


LoadBalancer 
------------
A loadbalancer type is an extension of the nodeport type, ie
- kubernetes will provision a load balancer 
- the load balancer will redirect traffic to the node ports across all the nodes, 
- clients will connect to the service using the load balancers ip address
- the load balancer will send to one of the nodes, on that dedicated port

To create a load balancer for your service, set the service type to LoadBalancer instead of NodePort
-the load balancer will have its own ip address

 eg:
 apiVersion: v1
 kind: Service
 metadata:
   name: kubia-loadbalancer
 spec:
   type: LoadBalancer           <-- service type
   ports:
   - port: 80
     targetPort: 8080           # don't specify a port
   selector:
     app: kubia

- note you do not specify a port for the loadbalancer unlike in the node port spec
  where yo specified a port
     - kubernetes will select a port (30817 as below) 

- to see the details of this svc:
  $ kubectl get svc kubia-loadbalancer
  NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP         PORT(S)        AGE
  kubia-loadbalancer   LoadBalancer   10.104.192.152   123.123.123.123     80:30817/TCP   1m

  -it will take some time to provision the loadbalancer
  -once the loadbalancer has been provisioned, you can hit the loadbalancer at its
   ip:port to hit the service
     $ curl 123.123.123.123:30817
       You've hit kubia-as4122

Routing of external clients using load balancer:
-client requests hit the ip of the load balancer on port 80
-the load balancer will route the request to one of the node ports on the implicit
 port, from there the node port will forward to one of the pods 
-as mentioned before, a load balancer is a type of node port with an additional
 infrastructure the load balancer
 (on minikube, the load balancer will never get provisioned)


Optimizing the network hops
---------------------------
in both the case of the node port (and the loadbalancer), the request always goes
from the client to a node port (via the load balancer) and then to a pod that
may not be running on the node as the node port, so this costs an extra network hop
-you can prevent this by configuring the service to redirect external 
 traffic only to pods running on the node that receive the connection
-set the externalTrafficPolicy field

 apiVersion: v1
 kind: Service
 metadata:
   name: kubia-loadbalancer
 spec:
   externalTrafficPolicy: Local          <--- optimize for network hops
   type: LoadBalancer
 ...

Drawbacks of Optimizing the network hops
----------------------------------------
3 main drawbacks:
* hanging connections
* uneven distribution
* requestor ip address lost

-with the externalTrafficPolicy configuration set to Local, 
 when an external connection is opened through
 the nodeport, the service proxy will choose a local running pod
    - if no such pod exists the connection will hang
    - so need to ensure the loadbalancer forwards connections only to 
      nodes where there is at least one such pod

-if the service pod distribution is not even on the nodes, the load balancer
 will load balance across the nodes, but then the pods handling the
 request are not evenly distributed, so the service handling will
 not be evenly distributed across the pods,
   eg, say there are 2 nodes load balanced, so each node gets 50%
       if one of the node in turn has 2 service pods, and the other node has 1 pod
       then the one pod node handles 50% of traffic, and the other two
       pods get 25% each, 

-when a client inside a cluster connects to the service, the pod can obtain the
 client ip from the packet, but using node ports, the packet source ip is 
 lost because SNAT (source network address translation) is performed
 on the packets, so the source ip is lost
   - this is an issue for some types of services, ex security services
   - the externalTrafficPolicy: Local will prevent the lost of the source ip
     because SNAT is not performed

Ingress Resources
-----------------
  ingress - the act of going in or entering, the right to enter.

In order for ingress resource to work, an ingress controller needs to be
running on the cluster
 - not all kubernetes environments do so

 - on minikube you can see the addons supported by:
   $ minikube addons list
     - addon-manager: enabled
     - coredns: enabled
     - dashboard: enabled
     - default-storageclass: enabled
     - efk: disabled
     - freshpod: disabled
     - heapster: disabled
     - ingress: enabled
     - kube-dns: disabled
     - metrics-server: disabled
     - nvidia-driver-installer: disabled
     - nvidia-gpu-device-plugin: disabled
     - registry: disabled
     - registry-creds: disabled
     - storage-provisioner: enabled

 - to enable the ingress addon:
    $ minikube addons enable ingress
      ingress was successfully enabled


 - usually the ingress service will be enabled on the kube-system namespace but
   not necessary, so you can list all pods across all namespaces to check
    $ kubectl get pods --all-namespaces
      NAMESPACE     NAME                                        READY     STATUS    RESTARTS   AGE
      ... 
      kube-system   nginx-ingress-controller-5984b97644-z7m5p   1/1       Running   0          2m
      ...

Creating an ingress resource
----------------------------
kubia-ingress.yml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia-ingress
spec:
  rules:
  - host: kubia.example.com             <--- this address will be serviced by this ingress
    http:                               <--- protocol serviced (http://kubia.example.com)
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport   <--- sent to this service
          servicePort: 80               <--- on this port


This creates an ingress service that handles all requests sent to
http://kubia.example.com
- and forwards to the kubia-nodeport on port 80

To find your ingress service
 $ kubectl get ingresses
   NAME      HOSTS               ADDRESS     PORTS     AGE
   kubia     kubia.example.com   10.0.2.15   80        4m

 
Then you need to add that host and address to your DNS server /etc/hosts
 so that it knows to resolve kubia.example.com to 10.0.2.15

then you can run curl http://kubia.example.com

and you will have accessed the service through ingress.

On cloud providers, the ingress controller actually provisions a load balancer behind
the scenes.


Advantage of Ingress Resource over Load Balancer
------------------------------------------------
Each LoadBalancer service requires it own load balancer with its own public ip.
An ingress service only requires one load balancer, and in fact the same
ingress service can be reused for multiple services
 - when a request is sent to the ingress service, the host.com/path in the
   request determine which service the request is forwarded to 

 - ingress service also operate at the application layer (http) stack, 
   so it can provide feature such as cookie-based session affinity
   which services cannot


Exposing multiple services
--------------------------
From the spec for ingress, you'll see that the rules and paths are lists, so
You can use one ingress service to expose 
 - multiple host  
 - multiple paths

Mapping different services to different paths of the same ingress host
 eg: ingress-multi-paths.yml

 apiVersion: extensions/v1beta1
 kind: Ingress
 metadata:
   name: kubia-multi-paths
 spec:
   rules:
     - host: kubia.example.com
       http:
         paths:
         - path: /kubia
           backend:
             serviceName: kubia
             servicePort: 80
         - path: /foo
           backend:
             serviceName: bar
             servicePort: 80

 this definition will route
   kubia.example.com/kubia to kubia service
   kubia.example.com/foo to bar service

 so clients will be able to reach two different services through a single ip address
 (the ingress controller)

Mapping different services to different hosts
 eg: ingress-multi-host.yml
 
 apiVersion: extensions/v1beta1
 kind: Ingress
 metadata:
   name: kubia-multi-paths
 spec:
   rules:
     - host: foo.example.com
       http:
         paths:
         - path: /foo
           backend:
             serviceName: foo
             servicePort: 80
     - host: bar.example.com
       http:
         paths:
         - path: /bar
           backend:
             serviceName: bar
             servicePort: 80

 This ingress service will be able to service both
    foo.example.com will be routed to foo
    bar.example.com will be routed to bar 
  depending on the Host header in the request.

 The DNS server will need to point foo.example.com and bar.example.com domain names
 to the Ingress controllers ip address


Configuring Ingress to Handle TLS Traffic
-----------------------------------------
-when a client opens a TLS connection to an ingress controller, the controller
 will terminate the TLS connection
   - the communication between the client and the controller is encrypted,
   - the communication between the controller and the back end pod is not
   - the application running in the pod doesn't need to support TLS, so it 
     can just accept http traffic, allowing the ingress controller to 
     take care of everything TLS related.
   - in order for the ingress controller to be able to handle TLS it needs
     to have an attached certificate + private key 
   - the certificate and private key are stored in a kubernetes resource called
     Secrets
   - the Secrets resource is referenced by the ingress manifest (yml file)

 Creating a Private Key and Certificate
 --------------------------------------
 creating the secrets consists of 3 steps
 1. generate teh private key
 2. generate the certificate
 3. generate the Secrets resource

 $ openssl genrsa -out tlskey 2048
   Generating RSA private key, 2048 bit long modulus
   ... (file called tls.key is created - the private key)

 $ openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com
   ... (file called tls.cert is created - the certificate)

 $ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
   secret/tls-secret created

   Now the private key and certificate are stored in a Secret called tls-secret.
   Now the Ingress resource can be updated to accept HTTPS requests by adding
   a configuration 
      secretName: tls-secret 
   to the ingress manifest (kubia-ingress-tls.yml)

   apiVersion: extensions/v1beta1
   kind: Ingress
   metadata:
     name: kubia
   spec:
     tls:                          <--- tls configuration
     - hosts:
       - kubia.example.com
       secretName: tls-secret      <--- secrets 
     rules:
     - host: kubia.example.com
       http:
         paths: 
         - path: /
           backend:
             serviceName: kubia-nodeport
             servicePort: 80

  so this tls configuration is for the kubia.example.com hostname
  the private key and cert are obtained from the tls-secret created above      

  if you already have the Ingress Resource created without TLS support, 
  you can apply it by invoking 
   $ kubectl apply -f kubia-ingress-tls.yml

  which will update the Ingress resource

 Once the TLS capable Ingress resource is ready, it can 
 accept https requests.


 Signing Certificates using CertificateSigningRequest resource
 -------------------------------------------------------------
 Kubernetes supports signing of certificate, of course the
 certificate signer component needs to be running in the cluster, otherwise
 signing and approving/denying certificates will have no effect.


Readiness Probes (Signaling when a Pod is Ready) 
================================================
A pod or application spinning up takes some time before it can accept
requests
- a readiness probe is needed in order for the pod to be able to
  signal that it is ready to accept requests
- kubernets is able to check if the app is running in the container
  by seeing if it responds to a simple GET  request at some
  defined endpoint
- the readiness probe implemetation detail is specific to the app,
  and the responsibility of the app developer

Like the liveness probe, there are 3 types
1. exec probe checks the processes exit code
2. HTTP GET probe sends a HTTP GEt request to the container, and the
   container responds if it is ready or not
3. TCP Socket probe opens a TCP connection to a specific port of 
   the container. if the connection is established, the container
   is considered ready


When a container is started, kubernetes can be configured to wait for
a configurable time to pass before the first readiness probe is performed.
After the delay, the readiness probe is checked periodically, if the probe
fails, the pod is removed from the service, until it is ready 
again, at which point it will be added back to the service

Unlike the liveness probe, a readiness probe will not be able to kill
of the service if it fails
Liveness probes keep pods healthy by killing of unhealthy containers and
 replacing them with new healthy ones
Readiness probes only make sure that the pod is ready to serve requests 

Adding a readiness probe
------------------------
You can add a readiness probe to a live replication controller definition:

 $ kubectl edit rc kubia

 apiVerison: v1
 kind: ReplicationController
 ...
 spec:
    ...
   template:
   ...
     spec:
       containers:
       - name: kubia
         image: luksa/kubia
         readinessProbe:
           exec:
             command:
             - ls
             - /var/ready
   ...

 This is a simple readiness probe.
 The ls command returns 0 if the file exists, non-zero exit code otherwise.
 If the file exists, the readiness probe succeeds.

 So you can toggle the results by creating the file.

 The readiness of a pod is captured by: 
  $ kubeclt get pods 
    NAME          READY     STATUS    RESTARTS   AGE
    kubia-b2xcx   0/1       Running   0          5d
    kubia-khn7s   0/1       Running   0          5d
    kubia-tcd2n   0/1       Running   0          5d

    Column READY shows all the pods that have passed the readiness probe.

    to create teh file you can run
  $ kubectl exec kubia-234vas -- touch /var/ready

  $ kubeclt get pods 
    NAME          READY     STATUS    RESTARTS   AGE
    kubia-b2xcx   1/1       Running   0          5d
    kubia-khn7s   1/1       Running   0          5d
    kubia-tcd2n   1/1       Running   0          5d

  
 In the real world, your readiness probe would be much more
 sophisticated.

 You can also define your pod label so that you can manually
 add and remove a pod with a simple label change
   eg: enabled=true
 as a label to your pod and label selector, then remove this 
 label or set to false when you want to take the pod out
 of service

Notes about Readiness Probes
===========================
-all pods should have readiness probes, otherwise they will
 be added to a service endpoint immediately, even if they
 are not ready

-kubernets will immediate remove the pod from all services as
 soon as the pod is deleted, so you do not need to fail 
 your readiness probe during the shutdown procedure.
 kubernetes knows to cleanly shutdown pods


