Replication & Controllers: Managed Pods
=======================================

in the real world, you would never create pods directly
-you would use either ReplicationControllers or Deployments, which then create the pod, plus manage the pod 
 directly for you

-when you create the pod directly, a cluster node is selected to run the pod
 then its containers are run in that pod, in that node
-kubernetes will then monitor the container, and restart them if they fail
-but if the whole node is lost, the pods on those nodes are lost and will not be replaced
 with new ones
-if you use replication controllers or similar, they will manage the pod for you
-understanding how kubernetes uses these to monitor the containers to see if the container
 is still alive, and how kubernetes restarts them if they are not

Keeping Pods Alive
==================
kubernetes uses a declarative model where you state the desired state, and then
let kubernetes do the necessary work to bring it to the desired state
-if you want a pod to run, you declare so, and then let kubernetes pick the
 worker node, and run the pods containers on that node

-if the container dies, kubelete on that node will run its containers and then from then
 on, keep them running as long as the pod exist
-if the processes main process crashes, the kubelet will restart the container
-if the application has a bug that crashes every once in a while, the kubelete will restart it
-simply running a app in kubernetes will give it the ability to heal itself

Liveness probes
===============
kubernetes checks if a container is alive through a liveness probe
-you can specify a liveness probe for each container in the pods spec
-kubernetes will periodically execute the probe an d restart the container if the probe fails

-there are 3 mechanisms kubernetes does liveness probes
   1. - http get on the containers ip address, port and path you specify in the manifest 
        as long as the GET returns a 2xx or 3xx response, the probe is considered successful
      - if the probe doesn't respond, or it returns some other response, it is considered a 
        failure, and the container will restart
  
   2. - TCP socket probe will try to open a TCP connection to the specified port of the 
        container. 
      - if the connection is established successfully, the probe is successful, otherwise
        the container fails
  
   3. - exec probe will execute an arbitrary command inside the container, checking the exit
        status
      - if the status code is 0, the probe is successful, otherwise all other codes are consider failures

HTTP Get liveness probe
=======================
  An example of a yaml manifest that specifies the liveness probe:

  apiVersion: 1
  kind: Pod
  metadata:
    name: kubia-livess
  spec:
    containers:
    - image: haja/kubia-unhealthy
      name: kubia
      livenessProbe:
        httpGet:
          path: /
          port: 8080 

Exit codes
==========
You can see why a container had to be restarted by looking at the content of the pods describe:
  > kubectl describe pod <pod name>

  In the section Last State: you will see that it was terminated, and the reason, and the error code.

  the error code is alway the sume of the signal number sent to the process and 128. 
  so if the error code is 137, the exit status is 137-128 = 9, which is SIGKILL, meaning the process was
  killed forcibly

  at the bottom of the descibe will be a listing of the events, 
  it should show that kubernetes detected that the container was unhealthy, so it was killed and re-created.
  when a container is killed, a new container is created, it is not the same container being restarted

Health check parameters
=======================
  the describe output will display information about the liveness probe such as:
   - the url/endpoint used
   - the delay
   - the timeout
   - the period, etc
   - failure count

   the delay = 0 shows that the probe begins immediately after the container started
   
   the timeout shows how long the probe will wait for a response before it timeouts and
   considers the probe failed
  
   the period shows how often the probe is run, evey x seconds

   the failure count is how many consecutive fails before the container gets restarted

 -all these parameters can be specified in the manifest

InitialDelay
============
    livenessProbe: 
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 15

    this specifies a delay of 15 seconds before the liveness probe starts
    you always want to specify a delay to account for your applications start up time, otherwise you
    will see that the probe kicks in right away, and it will fail, causing the process to be killed
    externally. This will be reflected in the exit code of 137 (SIGKILL) or 143 (SIGTERM).

    Rules for Effective Health checks
    =================================
    for production pods, you should always define a liveness probe 
      - a very simple probe will simply check if the server is responding
      - for more complex liveness probes, you can configure specific path (eg /health) and
        have the app perform internal status checks of vital components to ensure
        none of them has died or is unresponsive (eg database, jms, other services)
      - however, the health check should only check the internals of the app, and nothing influenced by
        external app. for example if app fails to connect to the database, the liveness probe should
        not fail. if the underlying database is the issue, restarting the app server/container will
        not fix the problem, so your app will just restart repeatedly
      - health checks should be fast (so that the timeout is small)
      - health checks should not use a lot of computational resources
        doing a health check should not slow down your container considerably 
      - you want your health check to not require authentication 
      - for java apps, you should make use of the http-get health check instead of
        an exec probe. the exec probe would spin up a whole new jvm which is heavy weight and 
        takes considerable computational resources

      - do not bother implementing retry loops in your probe, this is what the retry count is for
      - even when you set the threshold to 1, kubernetes will retry multiple times before considering
        it a failed attempt, so having your own retry loop is a wasted effort

     Node failure
     ============
      - the restart of a pod/container is done by the kubelete on the node hosting the pod, this is not
        done by the kubernetes control plane component
      - when the entire node fails, the kubernetes control plane must create replacements for
        all the pods in the node, however if you created the pod directly, these pods are not
        controlled by anything except the kubelete, and since the kubelete runs on the node directly,
        it will not be able to do anything if the node fails
      - to make sure your app is restarted on another node, they pod needs to be managed by a 
        replication controller or similar method
 

Replication Controller
======================
In order to have your app restart on another node in the event of a node failure, you need
to have your pod managed by a replication controller or other similar mechanism

replication controllers are kubernetes resource that ensures pods are always kept running
-if the pod disappears for any reason, ex if the node disappears from a cluster or if the
 node is evicted from a cluster, the replication controller will notice the nissing pod, and 
 create a replacement pod


Replication Controllers will constantly monitor the list of running pods and make sure the actual
number of pods of a "type" always match the desired number
 - if there are too few pods running, the rc will scale up to the desired number
 - if there are too many pods running, the rc will scale down 

Reasons why there may be more than the desired number of replicas
 - someone creates a pod of the same type
 - someone changes the type of an existing pod
 - someone descreases the desired number of pods

How Replication Controllers work
================================

Replication Controllers work based on the label selectors

a replication controller has 3 parts
-a label selector
-a replica count
-a pod template

-when you change the replica count, it will affect the existing pods
-when you change the label selector or the pod template, it will not affect the existing pod, 
 instead it will affect new pods

-changing the label selector will cause the existing pods to drop out of scope with the
 replication controller, so the rc will stop caring about them, and instead create a new set

-changing the pod template likewise will have no affect on the current pods, it will instead
 create new pods based on the new template when the need arises. the rc does not actually
 care about or monitor the pod contents after the pod has been created

Replication Controllers provide very powerful features like
-being able to create new pods when an existing one disappears
-when a node fails, any pods under control by rc will be replaced
-allows horizontal scaling both manual and automatic

Pods are never moved, but rather a new one is created.

Template for a Replication Controller
=====================================
  apiVersion: 1
  kind: ReplicationController
  metadata:
    name: kubia
  spec:
    replicas: 3
    selector:
      app: kubia
    template:
      metadata: 
        labels: 
          app: kubia
      spec:
        containers:
        - name: kubia
          image: haja/kubia
          ports:
            - containerPort: 8080

When such a manifest is submitted to the API Server, a replication controller
resource gets created with the name kubia
-the replica count is set to 3, 
-the 3 pods created will use the pod template provided with the label selector
 as specified "app=kubia"
-the pod labels will match the label selector of the replication controller
 otherwise the replciation controller will keep spinning up new pods
 until the replica matches the count and the label selector matches
-not specifying a label selector is also an option, in which case, it will be
 configured automatically from the labels in the pod template
-it is actually recommended that you not specify a label selector for the replicat
 controller, and instead let the labels be extracted from the pod template

Replication Controller commands
===============================

You can get the replication controller resources as
 > kubectl get rc

 this is analogous to get pod

Replication Controller in Action
=================================
once the replication controller has been created,
you should be able to see all the replica pods it has created
 
 > kubectl get pods
   
 will return all the pods associated with the rc

you can delete one of the pods and see that the rc will immediately
create a new pod
 > kubectl delete pod <pod name>
 
 the newly created pod will have a different status, and the terminating 
 pod will have a "Terminating" status

Replication Controller Information
==================================
You can see the rc information using
 > kubectl get rc

   NAME      DESIRED   CURRENT   READY     AGE
   kubia     3         3         3         39m

 which will display the rc, its desired, current count, the ready count and the age of teh rc

If you have multiple rc, and you just want one
 > kubectl get rc <rc name>


you can also describe the rc to get additional information about the rc
 > kubectl describe rc kubia

 this will give the details of the replication controller
 such as its:
  - name
  - namespace
  - selector
  - labels
  - annotations
  - replicas
  - the pod template (includes the container)
  - the events for that rc

How the Replication Controller works
====================================

When a pod managed by the rc is deleted, the rc will be notified.
When the rc is notified of a deletion, the rc will check the actual number
of pods against the desired count, and take appropriate action

Likewise when you lose an entire node, if the pods on the node are 
managed by replication controllers, then kubernetes will
automatically spawn new pods for you in different nodes
 - you can do this on a platform like google cloud
   where you can find the name of the node that a pod runs on 
   (kubectl get pods -o wide) will return the node
 - then disconnect that nodes network interface (sudo ifconfig eth0 down)

 - you can then list the nodes 
    > kubectl get node
      
    you will find one of the nodes status is NotReady.

 - once the node has been unreachable for several minutes, then the rc
   will immediately spin up new pods


Moving Pods in/out of scope of ReplicationController
====================================================
pods that are created by a rc are not bound to the rc in anyway,
rc manage the pods that match its label selector
 - if you change the labels of the pod, the pod no longer matches the
   rc, and the rc will stop seeing that pod. the pod has fallen out
   of scope. when a pod falls out of scope of the rc, the rc 
   will try to bring itself back to compliance with its replica count
 - in the same way, you can change an existing pods label so that it
   now matches the rc label selector
  
   assume you have a rc with pods labeled app=kubia
   > kubectl get pods --show-labels

  NAME          READY     STATUS    RESTARTS   AGE       LABELS
  kubia-kl997   1/1       Running   0          35m       app=kubia
  kubia-v6qjb   1/1       Running   0          51m       app=kubia
  kubia-xxtd5   1/1       Running   0          1h        app=kubia

  then you launch another pod:
   > kubectl create -f kubia.yaml

   > kubectl get pods --show-labels
     NAME          READY     STATUS    RESTARTS   AGE       LABELS
     kubia         1/1       Running   0          9s        <none>
     kubia-kl997   1/1       Running   0          37m       app=kubia
     kubia-v6qjb   1/1       Running   0          53m       app=kubia
     kubia-xxtd5   1/1       Running   0          1h        app=kubia 

  you see that the manually launched kubia has no label
 
  you can manually label that pod
   > kubectl label pod kubia app=kubia
     pod/kubia labeled

   > kubectl get pods --show-labels
   NAME           READY     STATUS        RESTARTS   AGE       LABELS
   kubia-kl997    1/1       Running       0          34m       app=kubia
   kubia-manual   1/1       Terminating   0          4m        app=kubia
   kubia-v6qjb    1/1       Running       0          50m       app=kubia
   kubia-xxtd5    1/1       Running       0          1h        app=kubia

   You should see that the pod is now labeled, but since the replica count is
   exceeded, one of the pods will terminate, which is what we see above

Changing the labels of managed pods
===================================
If you add a new label to existing pods managed by rc, it will not affect the
replica count or the pods, since the pods still are in scope of the rc label
selector
 
 > kubectl label pod kubia-kl997 type=special
 
 > kubectl get pods --show-labels

   NAME          READY     STATUS    RESTARTS   AGE       LABELS
   kubia-kl997   1/1       Running   0          44m       app=kubia,type=special
   kubia-v6qjb   1/1       Running   0          1h        app=kubia
   kubia-xxtd5   1/1       Running   0          1h        app=kubia


 you can change the label of a pod causing the pod to fall out of scope of the
 rc, in which cause, the pod will detach and no longer be managed by the
 rc, and then the rc will bring it back to its correct replica count
 
 > kubectl label pod kubia-kl997 app=fool --overwrite
   pod kubia-kl997 labeled

 > kubectl get pods --show-labels

   NAME          READY     STATUS    RESTARTS   AGE       LABELS
   kubia-7jtmh   1/1       Running   0          5s        app=kubia
   kubia-kl997   1/1       Running   0          55m       app=foo,type=special
   kubia-v6qjb   1/1       Running   0          1h        app=kubia
   kubia-xxtd5   1/1       Running   0          1h        app=kubia

   you see that the one pod label has changed, and a new pod with the rc scope
   has been created

Removing pods from the scope of a rc is handy when you want to debug/troubleshoot
a particular pod, for example a bug has been identified in that pod, or if the
pod started to behave eradictly. once down, you can delete the pod

Changing a Controllers pod template
===================================
A rc pod template can be modified at any time.
changing the pod template is like changing the mold of the rc,
it will only affect newly created pods, not the existing pods.
the existing pods will also continue to be in scope of the
rc since scope is determined by the label selectors

 to change the rc pod template:
 > kubectl edit rc kubia

 this will open the rc yaml definition in the default editor, allowing 
 you to change the pod template on the fly. once you save and exit, 
 the changes are persisted to the rc pod template and new pods
 will be as the pod template says

 you can add metadata (such as new label), change the container image,etc

Horizontally scaling a rc
=========================
you can scale up or down a rc by stating the new replica size
 
 > kubectl scale rc kubia --replicas=10

or you can edit the rc template

 > kubectl edit rc kubia

   then change the replica section
   saving and exiting the editor will save the rc info, and it will adjust accordingly.


Deleting a Replication Controller
=================================
there are two ways to delete a rc
 1. deleting the rc and also deleting the pods associated to it
 2. deleting just the rc and leaving the pods

when you delete the rc using 
 > kubectl delete rc kubia

   you will delete all pods and the rc associated

 
If you add a parameter --cascade=false, then only the rc will be deleted
 > kubectl delete rc kubia --cascade=false

   
Now if you want, you can create another rc with the same label selector and
the pods will fall right back into scope
 > kubectl create -f kubia-rc.yaml

ReplicaSet
==========
are the next generation of replication controllers
-behave almost exactly like a replication controller but has more expressive pod selector
-still common to see replication controllers in production
-replication controllers match pods according to certain label
-replication sets allow matching of pods that lack certain labels or pods that include certain label regardless of value
-replication sets can also match pods where label env=prduction and env=development at the same time, rc cannot do this
-replication sets can also match pods based on the presence of a label, regardless of its value ie env=*

Always use replica sets over replicationcontrollers, but you will find other people still using replciation controllers.

Defining a ReplicationSet
=========================
apiVersion: apps/v1beta2
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: haja/kubia


Expressive label selectors
==========================
-replicasets (rs) have a more expressive label selector than rc
-for example you can use "matchExpressions" for better expressions:

  selector:
    matchExpressions:
      - key: app
        operator: In
        values: 
          - kubia

 this selects pods containing a label with key "app" and hte value must be "kubia"

Operators that can be used in replication sets:
 - In - labels value must match one of the specified
 - NotIn - labels value must not match any of the specified values
 - Exists - Pod must include a label with the specified key (the value is not important).
            When using Exists, you should not specify the values field
 - DoesNotExist - Pod must not include a label with the specified key. The values property must
          not be specified

Deleting ReplicaSets
====================
Delete a ReplicaSet like you would a replication controller

 > kubectl delete rs kubia
 
DaemonSet
=========
A daemon set is used to run 1 pod on each of the nodes
-typical use case is for deploying infrastructure-related pods that need to run on each node,
 for example a log collector, resource monitort, kube-proxy process
-in a non-kubernetes cluster, these kinds of infrastructure would be run as part of the 
 node boot up process, but on kubernetes, you can take advantage of restarts, auto deployment etc

-a daemon set will create as many pods as there are nodes. So if a node is destroyed, 
 the daemon set will not restart the pod on another pod, but as soon as a new node is added,
 that pod will launch on the new node
-the daemon set has no concept of a replica count. it will only ensure that each node
 will have a such pod by making sure that the a pod running its pod selector is running
 on each node


Using DaemonSets to run pods on a subset of nodes
=================================================
-you can use a daemon set to run the pod on a certain subset of the nodes by specifying
 a node-selector property in the pod template, which is also part of the
 daemonset definition

-node-selectors are also used in pod selection

Labeling Nodes
---------------
 to retrieve the list of nodes
 > kubectl get node

 to add a label to a node
 > kubectl label node <node name> key=value
   
  eg  kubectl label node minikube disk=ssd

 > to change an existing label on a node, specify the --overwrite parameter
   > kubectl label node minikube disk=hdd --overwrite

 
Defining a DaemonSet
===================
apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
 

DaemonSet operators
===================
To create a daemon set, create the yaml file and then create a resource like all other resource
> kubectl create -f ds.yaml

To get the list of daemonsets
> kubectl get ds

  NAME          DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
  ssd-monitor   0         0         0         0            0           disk=ssd        17s

to delete a daemonset
> kubectl delete ds ssd-monitor

