Accessing Pod metadata + other resources from applications
----------------------------------------------------------
Topics discussed
-using DownwardAPI to pass info to containers
-using Kubernetes API Server to access metadata


many applications will need info about their environment, including details about:
 - themselves
 - other components in the cluster
 - 

Using DownwardAPI to pass meta data to container
------------------------------------------------
When your application needs access to data that you can configure ahead of time, you
typically will use configmap volumes
 
When your application needs access to data that is not known until the pod is created:
These metadata include:
 - pod ip
 - name name
 - pod labels and annotation
 - pod namespace
 - node name pod is running on
 - name of service account 
 - cpu/memory request for each container
 - cpu/memory limit of each container
 - pod labels
 - pod annotation


The best way to get these types of information is through a Downward API.

The DownwardAPI is used to pass in meta data about the pod + its environments.
There is two ways to access the downward API info in your application:
 - as environment variable
 - file in a downwardAPI volume


Exposing metadata through environment variables
-----------------------------------------------
To expose a pod/containers metadata through an environment variable, in the pod spec/manifest
you declare an environment variable and then use a fieldPath to populate the value.

eg see ch8/downward-api-env.yaml

 To create the resource, then see the environment variables of the pod
 > kubectl create -f downward-api-env.yaml
 > kubectl exec downward -- env

From the spec you should see that you need to define a divisor for resource fields so that
you get the value in the unit you need

All processes running inside the container will now be able to see/use those environment variables.

Container environment variables can only access the containers own resource limit and request.
A container env variable cannot access another containers resource limit/request.
(A downward volume can however be used to do this)


Exposing metadata through downward volume
-----------------------------------------
You can expose the metadata through a file instead by using a downward volume, which you
can then mount into your container.

For certain metadata, you must use a downwardAPI volume. These metadata include:
 - pod labels
 - pod annotation

eg: see ch8/downward-api-volume.yaml

To create the resource then see the volume content and the file content
 > kubectl create -f downward-api-volume.yaml
 > kubectl exec downward-volume -- ls -l /etc/download
 > kubectl exec downward-volume -- more /etc/download/labels

The reason that labels and annotations are only available through volume is because they can
be modified while a pod is running, so when they change, kubernetes will update the files,
allowing the pod to see the up to date value. You just have to write your application to
read the value in real time when they get updated.
 Environment variables on the other hand cannot be updated once they are created, which is
why pod labels and annotations are not exposed through env variables.

When you are exposing a container-level metadata (such as container resource limit/request)
you need to specify the name of the container whose resource you are referencing, because
there may be more than one container in your pod (containerName) but volumes are defined
at the pod level.

Downward volumes can be used to expose a 2nd containers resource limit/request to another
container (in contrast an env variable cannot) provided they are in the same pod.



Using the Kubernetes API Server
-------------------------------
The amount of meta data available through downward api is rather limited.
At most it limits you to the pod's own meta data, but there will be times when
you want to see data about the whole cluster or other pods. 

This is where the Kubernetes API Server comes in play.


To see the node ip of the master api service, run:
 > kubectl cluster-info
 Kubernetes master is running at https://192.168.99.100:8443

 To access this, you have to use a proxy 
 > kubectl proxy
 Starting to server on 127.0.0.1:8001
 
This proxy will start on localhost:8001
 So you can navigate to localhost:8001 (using curl or a browser)
 > curl localhost:8001
 {
  "paths": [
    "/api",
    "/api/v1",
    "/apis",
    "/apis/",
    "/apis/admissionregistration.k8s.io",
    "/apis/admissionregistration.k8s.io/v1beta1",
    "/apis/apiextensions.k8s.io",
    ... 
   ]
 }
 
 These paths correspond to the api groups and versions you specify in your resource definitions
 when creating the resources (pods, services etc)

 These paths are valid url you can call on the API Server to get additional information:
 eg:
  > curl localhost:8001/api
{
  "kind": "APIVersions",
  "versions": [
    "v1"
  ],
  "serverAddressByClientCIDRs": [
    {
      "clientCIDR": "0.0.0.0/0",
      "serverAddress": "192.168.99.101:8443"
    }
  ]
}


 And localhost:8001/api/v1 corresponds to the resources that were defined with
 spec: apiVersion: v1


 If you explore each of the resources you can discover:
  - groupings : collections that resources fall under
  - whether the resources are namespaced
  - the verbs that can be applied to each resource 
  - the kind 
  - REST endpoints for modifying their status
  

 You can get a listing of all jobs, the spec for a specific job, and much more
 from the API Server.


Talking to the API server from within a Pod
-------------------------------------------
Inside a pod, you won't have kubectl, so how do you talk to the API Server:
 - need to find the API Server ip
 - make sure that it is the API Server and not an impersonation
 - authentication


 To talk to the API Server you need a pod definition with a curl command, eg
   curl-pod.yaml

 > kubectl create -f curl-pod.yaml

 Then to get a shell on that created pod/container
 > kubectl exec curl -it -- bash
 root@curl:/#

 Then look for the env KUBBERNETES_SERVICE
 root@curl:/# env | grep KUBERNETES_SERVICE
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT_HTTPS=443

 These values should match up with the value you get from outside the pod:
 > kubectl get svc
 NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes           ClusterIP      10.96.0.1        <none>        443/TCP        111d
kubia-loadbalancer   LoadBalancer   10.104.192.152   <pending>     80:30817/TCP   90d
kubia-nodeport       NodePort       10.98.250.158    <none>        80:30123/TCP   91d


 In ch5, we learnt that each Service resource gets a DNS entry, you can curl to 
 https://kubernetes (assuming the port is 443).

 root@curl:/# curl https://kubernetes
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
 ...

 The error tells us we are missing the ca cert. Recall in the secrets (ch7) there
 was the default token which is mounted into each container at 
 /var/run/secrets/kubernetes.io/serviceaccount/

 root@curl:/# ls /var/run/secrets/kubernetes.io/serviceaccount/
 ca.crt    namespace   token

 the ca.crt is our certificate of the CA used to sign the kubernetes API server certificate.
 You need to verify the server's certificate is signed bby the CA. 
 
 > root@curl:/# curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.cert https://kubernetes
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {
    
  },
  "code": 403
}

 The response verifies the servers identity because the certificate was signed by the CA you
 trust. But you don't have authorization, which is why you get the 403.


 Next set the CURL_CA_BUNDLE so that you don't need to specify --cacert anymore
 >root@curl:/# export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.cert

 Now you should be able to curl the API Server:
 root@curl:/# curl https://kubernetes
 {  
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {

  },
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {

  },
  "code": 403
}

 You will still need to authenticate with the server, which requires the use of
 the default token, so lets export that as a env variable

 root@curl:/# export TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)

 You can then get the namespace from the secrets volume and get the pods from that namespace
 root@curl:/# NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace)
 root@curl:/# curl -H "Authorization: Bearer $TOKEN" https://kubernetes/api/vi/namespace/default/pods/
 
And you should get a response.



Ambassador containers
----------------------
An ambassador container pattern is one where a side car container runs alongside the main
container. This side car is responsible for communicating with the API Server and
it will take care of certificate and namespacing etc.

if your main application container needs to talk to the API Server, it will instead talk
to the side car (proxy) via HTTP and let the ambassador handle the HTTPS calls directly
to the API Server.
 eg: curl-with-ambassador.yaml

 To create the pod with the side car:
 > kubectl create -f curl-with-ambassador.yaml
 > kubectl exec -it curl-with-ambassador -c main bash
 root@curl-with-ambassador:/# curl localhost:8001
 {
   ...
 }

 This output should be the same as before, but now you don't have to deal with all the
 certificates etc.


Client Libraries
----------------
When you need to do more advanced queries to the Kubernetes API, you need one of 
the client libraries:
 - golang client: https://github.com/kubernetes/client-go
 - python clent: https://githubb.comkubernetes-incubator/client-python
 
As well there are clients in Java, Node, PHP, Ruby, Scala, Clojure, Perl
 

  
 
  
 













