All About Pods
--------------
-creating pod manifests
-delete pods
-working with pods
-describe pod
-working with Logs in Kubernetes
-setting up port-forwarding 
-labels (creating, showing, selecting, modifying)
-node selectors using labels
-pod annotations
-namespaces (create, working with, targeting objects to, switching, deleting objects in)

kubernetes basic unit is a pod
-containers are used to house single process applications
-pods are used to house 1 or more container
-kubernetes is about deploying pods, not containers
-pods are scaleable
-when to use multi-container pods and single container pods

-usually the containers in the same pod are related and dependent on each other in some way
-most use cases are 1 container in 1 pod
-containers in the same pod share the same IPC namespace, so can communicate through IPC,
 have the same IP address, have the same hostname


Why Do we need Pods
-------------------
-why not run multiple processes in a single container
   When multiple processes run on the same container, you have to manage 
     things like logging to different files, and how to ingest them
     or if they log to the same file, how do you figure out which process
     logs to where

   If a process crashed in a multi-process container, then you would need
     to manager restarting the process 
 
   With a single process container, you know that the logs in this container
     belong to the process.
   If the process crashes, you can just launch the whole container as you 
     did during the deployment. Since you already solved the deployment,
     you just reuse it, instead of having to solve for both the deployment
     of a container as well as the restart of processes.

-why not run containers directly in a node
   Since you cannot group multiple processes into a single container, then you 
   will need another higher construct that allows you to group related containers
   together, allowing you to manage them as a single unit. For example,
   some different processes share information through IPC. This is only possible 
   when the containers that house the processes run in the same machine or vm (or pod).
   But you don't want to deploy the containers on the host, because then when
   you move the container, you need to move several containers. What you 
   really want to do is move a single unit, and that unit will automatically
   move all its internal components, the containers.

**** Important****
So containers provide blast radius and process/resource isolation, but when
several processes need to function as a group, you need the blast radius, but
at the same time, the ability to move the entire group as one. Since they are
in separate containers, rather than moving 3 units, you want to move 1 unit 
and get all 3 at the same time, thats what pods provide.

Containers on the same pod can share IPC Namespace, allowing IPC communication.

-pods in the same cluster can communicate with other pods through their ip address.
 No NAT gateway exists between them.

Pods function much like a physical host or VM in the non-container world.
Processes running in the same pod are like processes running in the same host/vm,
but each process is now encapsulated in a container that provides a blast radius.

Organizing Pods
---------------
Think of pods as separate machines, bubt where each machine hosts only 1 app.
Pods are fairly light weight, so you don't want to cram alot of stuff onto the 
Pod. You can have as many Pods as you wish without incurring too much overhead.

So rather than stuffing too much in a pod, you should organize apps into multiple
pods, each containing only related components/processes.

Pods also allow you to scale.


in real world application you will create a pod through a manifest file
in either yaml or json format that gets posted to the Kubernets REST API
  up til now we have been creating pods through kubectl, but this is limiting
  because it only allows you to set small number of properties

Kubernets Resource Spec
-----------------------

to get the full yaml description of a pod
 > kubectl get pod -o yaml

   the main parts of the pod definition are:
   1 metadata
   2 spec 
   3 status

the meta data contains the 
   name, 
   namespace, 
   labels, 
   and other information about the pods

the spec contains the actual description of the pod content such as the pod 
   containers, 
   volumes

the status contains current information about the running pod, such as:
   condition of the pod
   description + status of each container
   pods internal ip address


A Simple YAML Descriptor of a Pod
---------------------------------

kubia.yaml is a simple pod only descriptor for a pod.

when you are creating your own spec for a pod to launch, you will never 
define the status, instead you will specify 
 - the api version
 - the kind of resource being defined (eg Pod or Service)
 - the spec 
 - the meta data

ports in the spec - when you define the spec of the pod, omitting ports will
have no effect on whether clients can connect or not. 
as long as your application is connected to the port, other clients can connect.
however it still makes sense to define the ports explicitly in the spec so that
when someone gets the resource description, they can tell that a port is 
open for connecting to, and explicitly defining ports will allow you to 
attach a name to the port, which will be useful

Pod Spec
--------

When creating a manifest (descriptor) you can lookup the kubernetes
reference doc to see which attributes are supported (kubernetes.io/docs/api)
or use the kubectl explain command:

> kubectl explain pods
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

...

For specifics of the spec
> kubectl explain pod.spec
KIND:     Pod
VERSION:  v1

RESOURCE: spec <Object>

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   activeDeadlineSeconds	<integer>
     Optional duration in seconds the pod may be active on the node relative to
     StartTime before the system will actively try to mark it failed and kill
     associated containers. Value must be a positive integer.
...


Creating A Kubernetes Resource from a Manifest file
---------------------------------------------------
To create a pod from a yaml/json spec
 > kubectl create -f <file spec>
 > kubectl create -f kubia-manual.yaml

   You should immediately see the response pod "kubia-manual" created

You should be able to get listing of pods by:
 > kubectl get pods 

   NAME           READY     STATUS    RESTARTS   AGE
   kubia-manual   1/1       Running   0          4s
   kubia-ww6sb    1/1       Running   1          4d

If the pod/container takes a while to start up, you'll see:

    NAME             READY     STATUS              RESTARTS   AGE
    kubia-liveness   0/1       ContainerCreating   0          19s

 Before it shows running status:

    NAME             READY     STATUS              RESTARTS   AGE
    kubia-liveness   1/1       Running   0          1m


To get the full description after creation depending on the format you are comfortable with:
 > kubectl get pod <pod name> -o yaml
 > kubectl get pod <pod name> -o json 

Deleting Pods
-------------
To delete a pod:
  > kubectl delete po kubia-liveness
pod "kubia-manual-v2" deleted

Working with Logs in Kubernetes
-------------------------------
Container applications usually log to standard output and standard error stream instead of
writing to a log file. The container will then redirect the streams to files allowing you
see the container logs by:

   eg: docker logs <container id>

If you wanted to see the log file container, you could ssh into the pod and then run
 >  docker logs <conatiner id>


Kubernetes allows you to see the log file contents by:
 > kubectl logs <pod-name>
 > kubectl logs kubia-manual

  logs are rotated daily and when they reach 10 MB. 
  kubectl logs only show current log file.


If you pod container multiple containers, you have to specify the container name
 > kubectl logs <pod-name> -c <container>
 > kubectl logs kubia-manual -c kubia

Kubernetes logs are in existence only as long as the pod still lives. If you
delete the pod, the logs associated with that pod is also deleted, so it is 
necessary for you to forward your logs to other tools like Splunk or 
set up centralized cluster-wide logging, which stores the logs into a 
central store. 


Port Forwarding
---------------
When you want to talk to a specific pod without going through a service, kubernetes
allows you to configure port forwarding to the pod

  > kubectl port-forward <pod-name> port1:port2

  will forward port1 of your machine to port2 of your pod
  to now in a new terminal, use curl localhost:port1 to hit the pods port2

  eg:
  > kubectl port-forward kubia-manual 8888:8080

  will forward your machines local port 8888 to port 8080 of the kubia-manual pod
  so now you can connect to your pod through the local port (in a different terminal since
  the port forwarder will open a port and not return til its closed)

  Allowing you to curl on the local port and hit the pod port:
  > curl localhost:8888
  You've hit kubia-manual
 

Labels - Organizing Pods
------------------------
Labels are a way of categorizing/tagging your pods.
 - groupings
 - team names
 - versions of apps (allowing multiple versions run in prod)
Labels allow you to organize your pods + other kubernetes objects.


Labels:
 - are arbitrary key value pairs that you assign to a resource
 - can be used to select for a subset of resources
 - resource can have many label 
 - the keys of the labels are unique within that resource
 - attach labels when you create resources, 
 - can be added/modified after the resource has been created, without the need to recreate the resource.
 - are defined in the meta data section of a resource spec

 see kubia-manual-with-labels.yaml for example of labels in a spec

Aside 
- a canary release is when you deploy a new version of an application next to the
  stable version, and only let a small fraction of users hit the canary version to see
  how it behaves before rolling it out to all users, preventing a large exposure if
  the release is unstable


Showing Labels/Selecting by Labels
----------------------------------
There are two parameter switches associated with labels:
  -l is the label selector
  -L is the label display

Labels will be visible to all person with access to that cluster
  Create the above kubia-manual-with-labels.yaml
  > kubectl create -f kubia-manual-with-labels.yaml

  Show the pod details with labels exposed
  > kubectl get pods --show-labels


Labels can also be used as selectors (-l), allowing filtering by labels
 
  -l option will filter and show only those pods that have the specified labels
     You can use -l in two ways:
       - with just a label 
       - with a label=value

     With -l label, you will get filter resources with that label regardless of the labels value
     With -l label=value, you will filter resources with the label and value specified

    > kubectl get pods -l creation_method,env
      Will get only pods that have creation_method and env labels (but will not show the labels)

    > kubectl get pod -l env 
      Will get pods with the env label regardless of value of env label

    > kubectl get pod -l creation_method=manual,team=banking
      Will get those pods with creation_method=manual and team=banking
      But will not show those labels

     To select pods that don't have a certain label, use single quotes and !
    > kubectl get pod -l '!env'
      Will select pods that do not have the env label

  -L option displays the specified label
    To get alls pods while displaying the labels provided as arguments to -L
    > kubectl get pods -L creation_method,env
      Will list all pods, and show all the creation method and env labels as columns

  -L -l will select by label, and display the labels
    > kubectl get pods -l team=banking -L team
      Will select pods labeled with team=banking, and then list then showing the team values


Select pods based on multiple values, use in|notin operator
  > kubectl get pod -l creation_method in (prod,dev)
  > kubectl get pod -l creation_method notin (prod,dev)


Modifying Pods Labels
-------------------------
Add a label to an existing pod using the "label" command:
  > kubectl label pod kubia-manual creation_method=manual
    this will add creation_method label of kubia-manual to be manual

Overwrite an existing label by:
  > kubectl label pod kubia-manual-v2 env=debug --overwrite


Using Labels to control pod scheduling
--------------------------------------
In general you don't want to specify which nodes a pod should be created on, and just allow the kubernetes
cluster to allocate pods based on their resource requirements (eg, x memory, y cpu, z ssd/hdd)
but once in a while you will want to control which node a pod will be scheduled on based on certain 
characteristics of the node and the application requirement, eg deploy to nodes using ssd, 
or deploy on nodes using GPU, etc

Labels can be attached to any resource, not just pods.
So when you set up a node that is not homogenous with the rest of the cluster, you can label that
node differently. And like pods you can select any resouce based on some label.
 
For example this node is marked as being a gpu node
> kubectl label node gke2-aruc-548fea gpu=true

like wise you can select nodes and have them display the labels 
> kubectl get node -l gpu=true -L gpu
  will retrieve nodes where gpu is true, as well as display the gpu label


to specify a pod get deployed to a certain node you need to specify in your yaml manifest
a node selector criteria in the spec selection
  see kubia-with-node-selector.yaml

  apiVersion: v1
  kind: Pod
  metadata:
     name: kubia-gpu
  spec:
     nodeSelector: 
       gpu: "true"
     containers:
     - image: haja/kubia
       name: kubia
       ports:
       - containerPort: 8080
         protocol: TCP


Targeting pod deployment to specific node
-----------------------------------------
Each node has a unique label with the key kubernetes.io/hostname which is the actual hostname of the node.
You could potentially use this as a label selector to control your deployment, but in general doing so
would be bad practice because if such a node label does not exist, then the pod will become
unscheduled. 

Don't think in terms of individual nodes, but rather, node features you want to target


Pod Annotations
---------------
Pods and other objects can contain annotations. 
Annotations are key value pairs, similar to labels, but they are not meant to hold identifying information.
They cannot be used to group objects as there are no such thing as annotation selectors.

Annotations hold much more information then labels, and are mainly used by tools.
Some tools will add annotations, and some annotations get added by users.

Annotations are common used when a new feature gets added to kubernetes, before the API is clear
about the feature, as a way to document a feature, and then once the feature is agreed upon by
the community, new fields are added, and the annotation gets deprecated.

A good use of annotation is to add description to each pod or api object so that everyone using the
cluster can quickely get information about each object.

It is a good idea to use namespacing when annotating so that collisions doen't happen
   gecko.com/annotationkey=annotationvalue

   see kubia-manual-with-annotation.yaml

An objects annotation will be in the resources metadata section.
To view the objects annotation, you need to objects yaml/json manifest
 > kubectl get pod <pod> -o yaml 

to add an annotation to an existing pod
 > kubectl annotate pod <pod-name> company.com/someannotaion="somevalue"

you can also view the added annotation by kubectl describe
 > kubectl describe pod <pod-name>

You can also see the image that a pod is running by
 > kubectl describe pod <pod name> | grep Image


Kubernetes Namespaces
---------------------
-used for splitting objects into non-overlapping groups
-ns provide scope for object names
-by using namespaces, you can have objects with the same name

Why do we need namespaces
-allows for multi-tenancy, 
-or for separating environments eg qa, dev, e2e, etc.
-or some other functional grouping
-resources can then be named the same, but in these different ns, they will not overlap/clash 
 with each other

-namespaces do not provide isolation of the running objects. 
-namespace only provides logical group distinction,
-objects can still communicate across namespaces if there is not ioslation.

Some objects are not namespaced, for example the Node resource is universal
Nodes do not reside in a namespace, they are global and not tied to a namespace.


Commands for Namespaces
-----------------------
-to get all the name spaces
  > kubectl get ns

You will see that there are multiple namespaces
- default namespace, 
- kube-public 
- kube-system namespace.

By default, your commands will operate on the default namespace.

-to get objects within a certain namepace
  > kubectl get pod --namespace kube-system

    Will return all pods within the kube-system namepsace. 


-Objects core to kubernetes runs in their own namepsace, and arent returned when 
 you are running your standard commands (which work for the default namespace), 
 otherwise kube-system objects would clutter all your return values

-Namespaces can also be used to organize different user objects so that
 they don't see each others objects, or collide object names (ex sample)

Creating namespaces
-------------------
Two ways to create namespaces
-manifest file
-kubectl command


-like other resource, they can be created via manifest files
 (all kubernetes objects have a corresponding API object that allows you
  to create/read/update/delete by posting yaml manifest to the server)

 eg custom-namespace.yaml
 
   > kubectl create -f custom-ns.yaml
     namespace "custom-namespace" created

-or with a command:
   > kubectl create namespace my-namespace

There are rules for object names (RFC 1035 - domain names) 
-namespaces may not conatin dots.

Switching namespace (context)
-----------------------------
To find your current context
 > kubectl config current-context
   minikube
   
   or if you are in your cluster, it will reflect your current ns
   eg admin@t360-ns

You can switch between namespaces with the command

 kubectl config set-context $(kubectl config current-context) --namespace <namespace to switch to>
   eg

 > kubectl config set-context $(kubectl config current-context) --namespace ass-kick-ns

   which switch to the namespace 'ass-kick-ns' and all kubectl commands will execute in that
   context, until you switch back to default


Creating resources in namespaces
--------------------------------
Objects can be created in specified ns by:
 - specifying the namespace as part of the metadata section 
 - targeting the namespace with --namespace or -n
 - create the ns and then use the ns in the same manifest
 
 kubia-manual-custom-namespace.yaml declares its own namespace
 to confirm it was created in ninja-ns, first get the namespaces
 to confirm the various ns
   > kubectl get ns
     // should show you all the ns, including default, kube-system, kube-public
     // and others you created

 then create the resource:
   > kubectl create -f kubia-manual-custom-namespace.yaml


 If the targeted ns does not yet exist, you will get an error.
   -either create the ns manually or 
   -you can add the ns declaration to the manifest
   (see kubia-with-create-namespace.yaml)

 To target creation of object into a namespace, use the parameter in the create command
   > kubectl create -f my-manifest.yaml --namespace custom-namespace
   > kubectl create -f my-manifest.yaml -n custom-namespace


When working objects in a namespace, if you don't specify the namespace, it will be the default
namespace, otherwise you need to specify it as part of your command parameter
   > kubectl get pods --namespace <ns>
     Will retrieve the pods from specified namespace, regardless of which ns you are currently in

  The same parameter switch should work with other commands also



Deleting objects
----------------
You are always working in the default namespace, so your commands are
targeted to that namespace.

-to delete objects in other namespaces you should provide the ns as a parameter via 
 --namespace 
 -n
 
-or switch context to that namespace


Deleting Namespaces
-------------------
A namespace is just another resource, so you should be able to delete them
 > kubectl delete ns <name>


Deleting Pods
-------------
When you tell Kubernetes to delete a pod:
- kubernetes will send a SIGTERM signal to the process and wait default (30 secs) 
  time to allow the process to gracefully shut down. 
- if not the pods is not dead by then, then it will send a SIGKILL signal
- make sure your processes respond to SIGTERM to ensure graceful exits
  (means you should application logic should handle this gracefully)

To delete pods by name:
  > kubectl delete pod <pod name>, <pod name2>, ...


Use the label selector to delete pods with that label
  > kubectl delete pod -l creation_method=manual
    will delete all pods with that label and value.

Delete a namespace, and all resources in the namespace automatically
  > kubectl delete ns <namespace>

Delete all pods in a namespace but not the namespace (what abbout replica sets?)
  > kubectl delete pod --all
    This will just delete pods, but the rs may relaunch the pods again.

Delete all rc/rs/pods/svc with
  > kubectl delete all --all
    This will ensure that new pods don't spin up again.
    This will delete the Kubernetes Service object, but that will get recreated automatically.
    This will not delete secrets or other config maps
    

Deleting replication controllers
--------------------------------
anything that was created by a replication controller will attempt to scale up to its desired size
after a delete all command, so you have to delelete the replication controller as well
> kubectl delete all --all
 (this will delete services, rc, pods but will not delete secrets)

Or you can delete the replication controller manually

> kubectl get rc
  NAME     DESIRED      CURRENT     READY AGE
  kubia    1            1           1     6d

> kubectl delete rc kubia
  replicationcontroller "kubia" deleted
