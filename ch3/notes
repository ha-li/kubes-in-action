All About Pods
--------------
kubernetes basic unit is a pod
-containers are used to house single process applications
-pods are used to house 1 or more container
-kubernetes is about deploying pods, not containers
-pods are scaleable
-when to use multi-container pods and single container pods
-creating pods


-usually the containers in the same pod are related and dependent on each other in some way
-most use cases are 1 container in 1 pod
-containers in the same pod share the same IPC namespace, so can communicate through IPC,
 have the same IP address, have the same hostname


Why Do we need Pods
-------------------
-why not run multiple processes in a single container
   When multiple processes run on the same container, you have to manage 
     things like logging to different files, and how to ingest them
     or if they log to the same file, how do you figure out which process
     logs to where

   If a process crashed in a multi-process container, then you would need
     to manager restarting the process 
 
   With a single process container, you know that the logs in this container
     belong to the process.
   If the process crashes, you can just launch the whole container as you 
     did during the deployment. Since you already solved the deployment,
     you just reuse it, instead of having to solve for both the deployment
     of a container as well as the restart of processes.

-why not run containers directly in a node
   Since you cannot group multiple processes into a single container, then you 
   will need another higher construct that allows you to group related containers
   together, allowing you to manage them as a single unit. For example,
   some different processes share information through IPC. This is only possible 
   when the containers that house the processes run in the same machine or vm (or pod).
   But you don't want to deploy the containers on the host, because then when
   you move the container, you need to move several containers. What you 
   really want to do is move a single unit, and that unit will automatically
   move all its internal components, the containers.

**** Important****
So containers provide blast radius and process/resource isolation, but when
several processes need to function as a group, you need the blast radius, but
at the same time, the ability to move the entire group as one. Since they are
in separate containers, rather than moving 3 units, you want to move 1 unit 
and get all 3 at the same time, thats what pods provide.

Containers on the same pod can share IPC Namespace, allowing IPC communication.

-pods in the same cluster can communicate with other pods through their ip address.
 No NAT gateway exists between them.

Pods function much like a physical host or VM in the non-container world.
Processes running in the same pod are like processes running in the same host/vm,
but each process is now encapsulated in a container that provides a blast radius.

Organizing Pods
---------------
Think of pods as separate machines, bubt where each machine hosts only 1 app.
Pods are fairly light weight, so you don't want to cram alot of stuff onto the 
Pod. You can have as many Pods as you wish without incurring too much overhead.

So rather than stuffing too much in a pod, you should organize apps into multiple
pods, each containing only related components/processes.

Pods also allow you to scale.


in real world application you will create a pod through a manifest file
in either yaml or json format that gets posted to the Kubernets REST API
  up til now we have been creating pods through kubectl, but this is limiting
  because it only allows you to set small number of properties

Kubernets Resource Spec
=======================

to get the full yaml description of a pod
 > kubectl get pod -o yaml

   the main parts of the pod definition are:
   1 metadata
   2 spec 
   3 status

the meta data contains the 
   name, 
   namespace, 
   labels, 
   and other information about the pods

the spec contains the actual description of the pod content such as the pod 
   containers, 
   volumes

the status contains current information about the running pod, such as:
   condition of the pod
   description + status of each container
   pods internal ip address

when you are creating your own spec for a pod to launch, you will never 
define the status, instead you will specify 
 - the api version
 - the kind of resource being defined (eg Pod or Service)
 - the spec 
 - the meta data

ports in the spec - when you define the spec of the pod, omitting ports will
have no effect on whether clients can connect or not. 
as long as your application is connected to the port, other clients can connect.
however it still makes sense to define the ports explicitly in the spec so that
when someone gets the resource description, they can tell that a port is 
open for connecting to, and explicitly defining ports will allow you to 
attach a name to the port, which will be useful

Pod Spec
========

to outline the pod spec
 > kubectl explain pods

for specifics of the spec
 > kubectl explain pod.spec

Creating A Kubernetes Resource from a Manifest file
===================================================

to create a pod from a yaml/json spec
 > kubectl create -f <file spec>
 > kubectl create -f kubia-manual.yaml

 You should immediately see the response 
  pod "kubia-manual" created

  > kubectl get pods will return existing pods and the newly created pod

    NAME           READY     STATUS    RESTARTS   AGE
    kubia-manual   1/1       Running   0          4s
    kubia-ww6sb    1/1       Running   1          4d

  if the pod/container takes a while to start up, you'll see:

    NAME             READY     STATUS              RESTARTS   AGE
    kubia-liveness   0/1       ContainerCreating   0          19s

    before it shows running status:

    NAME             READY     STATUS              RESTARTS   AGE
    kubia-liveness   1/1       Running   0          1m


to get the full description after creation depending on the format you are comfortable with:

  > kubectl get pod <pod name> -o yaml
  > kubectl get pod <pod name> -o json 


Working with logs in Kubernetes
===============================

Container applications usually log to standard output and standard error stream instead of
writing to a log file. The container will then redirect the streams to files allowing you
see the container logs by:
   eg: docker logs <container id>


If you wanted to see the log file container, you could ssh into the pod and then run
    docker logs <conatiner id>

but kubernetes allows you to see the log file contents by:

 > kubectl logs <pod-name>
   kubectl logs kubia-manual

  logs are rotated daily and when they reach 10 MB. 
  kubectl logs only show current log file.


if you pod container multiple containers, you have to specify the container name
 > kubectl logs <pod-name> -c <container>
   kubectl logs kubia-manual -c kubia

Kubernetes logs are in existence only as long as the pod still lives. If you
delete the pod, the logs associated with that pod is also deleted, so it is 
necessary for you to forward your logs to other tools like Splunk.


Port Forwarding
===============
When you want to talk to a specific pod without going through a service, kubernetes
allows you to configure port forwarding to the pod

 > kubectl port-forward <pod-name> port1:port2
   will forward port1 of your machine to port2 of your pod
   to now in a new terminal, use curl localhost:port1 to hit the pods port2

   eg:
   > kubectl port-forward kubia-manual 8888:8080
     will forward your machines local port 8888 to port 8080 of the kubia-manual pod
     
     so now you can connect to your pod through the local port (in a different terminal since
     the port forwarder will open a port and not return til its closed)
 

Labels - Organizing Pods
========================

In a true production environment, you will be managing thousands of pods, so there
is a need to organize them for fast retrieval. Labels help you categorize pods
so that you can retrieve and affect pods based on their labels.

- you can have multiple versions (v1, v2, ... beta, canary)
- different microservices

labels are the equivalent of aws tags.
-labels are arbitrary key value pairs that you can assign to a resource, which can 
 then be used to select for a subset of resources using the label

-a resource can have more than one label as long as the keys of the labels are unique 
 within that resource
-usually attach a label to a resource when you create them, but labels can be
 added/modified even after the resource has been created, without the need to 
 recreate the resource.

 aside - a canary release is when you deploy a new version of an application next to the
 stable version, and only let a small fraction of users hit the canary version to see
 how it behaves before rolling it out to all users, preventing a large exposure if
 the release is unstable

-when defining the resource manifest, the labels are affixed in the meta data section 

Showing Labels
==============
-labels will be visible to all person with access to that cluster
 > kubectl get pods --show-labels

-can also specify the labels you are interested in, and all pods will be returned with
 labels in special columns
 > kubectl get pods -L creation_method,env
   - the capital -L option will return all pods, displaying the labels provided as arguments to -L

-the -l option will select and display only those pods with the provided labels

-if you add both -l and -L you will select by label, and display the labels
  > kubectl get pods -l creation_method -L creation_method
    will select pods labeled with creation_method, and then list then showing label creation method

Modifying Existing Labels
=========================
-you can also add labels to an existing pods
 > kubectl label pod kubia-manual creation_method=manual
   this will add creation_method label of kubia-manual to be manual

-if the pod already has that label, then you have to specify to overwrite it
 > kubectl label pod kubia-manual-v2 env=debug --overwrite


Label Selectors
===============
-to select pods based on labels with a specific value, set the =value on the label selector
 > kubectl get pod -l creation_method=manual

-to select pods with a certain label specified regardless of value
 > kubectl get pod -l env 

-to select pods on a label and also display the labels, you combine both -l and -L
 > kubectl get pod -l env -L creation_method,env

-to select pods that don't have a certain label, use single quotes and !
 > kubectl get pod -l '!env'

-to select pods based on multiple values, use in|notin operator
 > kubectl get pod -l creation_method in (prod,dev)
 > kubectl get pod -l creation_method notin (prod,dev)


Using Labels to control pod scheduling
======================================
In general you don't want to specify which nodes a pod should be created on, and just allow the kubernetes
cluster to allocate pods based on their resource requirements (eg, x memory, y cpu, z ssd/hdd)
but once in a while you will want to control which node a pod will be scheduled on based on certain 
characteristics of the node and the application requirement, eg deploy to nodes using ssd, 
or deploy on nodes using GPU, etc

Labels can be attached to any resource, not just pods.
So when you set up a node that is not homogenous with the rest of the cluster, you can label that
node differently. And like pods you can select any resouce based on some label.
 
For example this node is marked as being a gpu node
> kubectl label node gke2-aruc-548fea gpu=true

like wise you can select nodes and have them display the labels 
> kubectl get node -l gpu=true -L gpu
  will retrieve nodes where gpu is true, as well as display the gpu label


to specify a pod get deployed to a certain node you need to specify in your yaml manifest
a node selector criteria in the spec selection
  apiVersion: v1
  kind: Pod
  metadata:
     name: kubia-gpu
  spec:
     nodeSelector: 
       gpu: "true"
     containers:
     - image: haja/kubia
       name: kubia

Targeting pod deployment to specific node
========================================= 
Each node has a unique label with the key kubernetes.io/hostname which is the actual hostname of the node.
You could potentially use this as a label selector to control your deployment, but in general doing so
would be bad practice because if such a node label does not exist, then the pod will become
unscheduled. 

Don't think in terms of individual nodes, but rather, node features you want to target


Pod Annotations
---------------
Pods and other objects can contain annotations. 
Annotations are key value pairs, similar to labels, but they are not meant to hold identifying information.
They cannot be used to group objects as there are no such thing as annotation selectors.

Annotations hold much more information then labels, and are mainly used by tools.
Some tools will add annotations, and some annotations get added by users.

Annotations are common used when a new feature gets added to kubernetes, before the API is clear
about the feature, as a way to document a feature, and then once the feature is agreed upon by
the community, new fields are added, and the annotation gets deprecated.

A good use of annotation is to add description to each pod or api object so that everyone using the
cluster can quickely get information about each object.

It is a good idea to use namespacing when annotating so that collisions doen't happen
   gecko.com/annotationkey=annotationvalue

An objects annotation will be in the resources metadata section.
To view the objects annotation, you need to objects yaml/json manifest
 > kubectl get pod <pod> -o yaml 

to add an annotation to an existing pod
 > kubectl annotate pod <pod-name> company.com/someannotaion="somevalue"

you can also view the added annotation by kubectl describe
 > kubectl describe pod <pod-name>


Kubernetes Namespaces
=====================

There will be times when you want to be able to split your objects into separate non-overlapping groups
so that you only perform operations inside one group at a time. This is where namespacing comes into play.
Namespacing provides a scope for object names.

Namespaces can also be used for multi-tenancy, or for separating environments eg qa, dev, e2e, etc.

So within different namespaces, objects can have the same name -- this may be useful.
For example in complex systems, you want to split your components into smaller distinct groups.
For example prod/pre-prod environments. So if you have a qal namespace and a prod namespace, the
objects name have the same name in each of those namespaces.

Nodes do not reside in a namespace, they are global and not tied to a namespace.

Kubectl commands for namespaces

-to get all the name spaces
 > kubectl get ns

you will see that there is a default namespace, a kube-public and kube-system namespace.

-to get objects within a certain namepace
> kubectl get pod --namespace kube-system

  will return all pods within the kube-system namepsace. It would make sense that objects used to 
run kubernetes will reside in their own namepsace, and don't get returned when you are running
your standard commands (which work for the default namespace), otherwise your the kube-system 
objects would clutter all your return values

Creating namespaces
===================
Namespaces are just another kubernetes resource, so they get created just like any other object, via
a manifest file:
 
   apiVersion: v1
   kind: Namespace
   metadata:
      name: custom-namespace

   > kubectl create -f custom-ns.yaml
     namespace "custom-namespace" created


You can also create namesapces with a command:
 > kubectl create namespace my-namespace

There are rules for object names (RFC 1035 - domain names) but namespaces may not conatin dots.

Creating resources in namespaces
=================================
When creatign objects, if you want to target a namespace, you can either declare the
namespace in the object manifest, or in the create command
> kubectl create -f my-manifest.yaml --namespace custom-namespace

When working objects in a namespace, if you don't specify the namespace, it will be the default
namespace, otherwise you need to specify it as part of your command.

Deleting objects
================
Likewise, if you want to delete objects in a certain namespace, you need to provide
the namespace as a parameter, other wise it assumes it is in default.

Switching namespaces
====================
You can switch between namespaces with the command
 > kubectl config set-context $(kubectl config current-context) --namespace <namespace to switch to>
 > kubectl config set-context $(kubectl config current-context) --namespace ass-kick-ns
    which switch to the namespace 'ass-kick-ns' and all kubectl commands will execute in that
    context, until you swtich back to default


Namespaces do not provide isolation of the running objects. 
Namespace only provides logical group distinction,
but objects can still communicate across namespaces if there is not ioslation.

Deleting Namespaces
===================
A namespace is just another resource, so you should be able to delete them
 > kubectl delete ns <name>

Deleting Pods
=============

To delete pods
> kubectl delete pod <pod name>, <pod name2>, ...

Kubernetes will send a SIGTERM signal to the process and wait for (30 secs) default time to allow the process
to gracefully shut down. If not, then it will send a SIGTERM signal

You can use the label selector to delete pods with that label
> kubectl delete pod -l creation_method=manual
 will delete all pods with that label and value.

You can delete an entire namespace, and all resources in the namespace automatically
> kubectl delete ns <namespace>

You delete all pods in a namespace
> kubectl delete pod --all

Deleting replication controllers
================================
anything that was created by a replication controller will attempt to scale up to its desired size
after a delete all command, so you have to delelete the replication controller as well
> kubectl delete all --all
 (this will delete services, rc, pods but will not delete secrets)

Or you can delete the replication controller manually

> kubectl get rc
  NAME     DESIRED      CURRENT     READY AGE
  kubia    1            1           1     6d

> kubectl delete rc kubia
  replicationcontroller "kubia" deleted
