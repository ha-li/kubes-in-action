All About Pods
--------------
-creating pod manifests
-delete pods
-working with pods
-working with Logs in Kubernetes
-setting up port-forwarding 
-labels (creating, showing, selecting, modifying)

kubernetes basic unit is a pod
-containers are used to house single process applications
-pods are used to house 1 or more container
-kubernetes is about deploying pods, not containers
-pods are scaleable
-when to use multi-container pods and single container pods

-usually the containers in the same pod are related and dependent on each other in some way
-most use cases are 1 container in 1 pod
-containers in the same pod share the same IPC namespace, so can communicate through IPC,
 have the same IP address, have the same hostname


Why Do we need Pods
-------------------
-why not run multiple processes in a single container
   When multiple processes run on the same container, you have to manage 
     things like logging to different files, and how to ingest them
     or if they log to the same file, how do you figure out which process
     logs to where

   If a process crashed in a multi-process container, then you would need
     to manager restarting the process 
 
   With a single process container, you know that the logs in this container
     belong to the process.
   If the process crashes, you can just launch the whole container as you 
     did during the deployment. Since you already solved the deployment,
     you just reuse it, instead of having to solve for both the deployment
     of a container as well as the restart of processes.

-why not run containers directly in a node
   Since you cannot group multiple processes into a single container, then you 
   will need another higher construct that allows you to group related containers
   together, allowing you to manage them as a single unit. For example,
   some different processes share information through IPC. This is only possible 
   when the containers that house the processes run in the same machine or vm (or pod).
   But you don't want to deploy the containers on the host, because then when
   you move the container, you need to move several containers. What you 
   really want to do is move a single unit, and that unit will automatically
   move all its internal components, the containers.

**** Important****
So containers provide blast radius and process/resource isolation, but when
several processes need to function as a group, you need the blast radius, but
at the same time, the ability to move the entire group as one. Since they are
in separate containers, rather than moving 3 units, you want to move 1 unit 
and get all 3 at the same time, thats what pods provide.

Containers on the same pod can share IPC Namespace, allowing IPC communication.

-pods in the same cluster can communicate with other pods through their ip address.
 No NAT gateway exists between them.

Pods function much like a physical host or VM in the non-container world.
Processes running in the same pod are like processes running in the same host/vm,
but each process is now encapsulated in a container that provides a blast radius.

Organizing Pods
---------------
Think of pods as separate machines, bubt where each machine hosts only 1 app.
Pods are fairly light weight, so you don't want to cram alot of stuff onto the 
Pod. You can have as many Pods as you wish without incurring too much overhead.

So rather than stuffing too much in a pod, you should organize apps into multiple
pods, each containing only related components/processes.

Pods also allow you to scale.


in real world application you will create a pod through a manifest file
in either yaml or json format that gets posted to the Kubernets REST API
  up til now we have been creating pods through kubectl, but this is limiting
  because it only allows you to set small number of properties

Kubernets Resource Spec
-----------------------

to get the full yaml description of a pod
 > kubectl get pod -o yaml

   the main parts of the pod definition are:
   1 metadata
   2 spec 
   3 status

the meta data contains the 
   name, 
   namespace, 
   labels, 
   and other information about the pods

the spec contains the actual description of the pod content such as the pod 
   containers, 
   volumes

the status contains current information about the running pod, such as:
   condition of the pod
   description + status of each container
   pods internal ip address


A Simple YAML Descriptor of a Pod
---------------------------------

kubia.yaml is a simple pod only descriptor for a pod.

when you are creating your own spec for a pod to launch, you will never 
define the status, instead you will specify 
 - the api version
 - the kind of resource being defined (eg Pod or Service)
 - the spec 
 - the meta data

ports in the spec - when you define the spec of the pod, omitting ports will
have no effect on whether clients can connect or not. 
as long as your application is connected to the port, other clients can connect.
however it still makes sense to define the ports explicitly in the spec so that
when someone gets the resource description, they can tell that a port is 
open for connecting to, and explicitly defining ports will allow you to 
attach a name to the port, which will be useful

Pod Spec
--------

When creating a manifest (descriptor) you can lookup the kubernetes
reference doc to see which attributes are supported (kubernetes.io/docs/api)
or use the kubectl explain command:

> kubectl explain pods
KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

...

For specifics of the spec
> kubectl explain pod.spec
KIND:     Pod
VERSION:  v1

RESOURCE: spec <Object>

DESCRIPTION:
     Specification of the desired behavior of the pod. More info:
     https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status

     PodSpec is a description of a pod.

FIELDS:
   activeDeadlineSeconds	<integer>
     Optional duration in seconds the pod may be active on the node relative to
     StartTime before the system will actively try to mark it failed and kill
     associated containers. Value must be a positive integer.
...


Creating A Kubernetes Resource from a Manifest file
---------------------------------------------------
To create a pod from a yaml/json spec
 > kubectl create -f <file spec>
 > kubectl create -f kubia-manual.yaml

   You should immediately see the response pod "kubia-manual" created

You should be able to get listing of pods by:
 > kubectl get pods 

   NAME           READY     STATUS    RESTARTS   AGE
   kubia-manual   1/1       Running   0          4s
   kubia-ww6sb    1/1       Running   1          4d

If the pod/container takes a while to start up, you'll see:

    NAME             READY     STATUS              RESTARTS   AGE
    kubia-liveness   0/1       ContainerCreating   0          19s

 Before it shows running status:

    NAME             READY     STATUS              RESTARTS   AGE
    kubia-liveness   1/1       Running   0          1m


To get the full description after creation depending on the format you are comfortable with:
 > kubectl get pod <pod name> -o yaml
 > kubectl get pod <pod name> -o json 

Deleting Pods
-------------
To delete a pod:
  > kubectl delete po kubia-liveness
pod "kubia-manual-v2" deleted

Working with Logs in Kubernetes
-------------------------------
Container applications usually log to standard output and standard error stream instead of
writing to a log file. The container will then redirect the streams to files allowing you
see the container logs by:

   eg: docker logs <container id>

If you wanted to see the log file container, you could ssh into the pod and then run
 >  docker logs <conatiner id>


Kubernetes allows you to see the log file contents by:
 > kubectl logs <pod-name>
 > kubectl logs kubia-manual

  logs are rotated daily and when they reach 10 MB. 
  kubectl logs only show current log file.


If you pod container multiple containers, you have to specify the container name
 > kubectl logs <pod-name> -c <container>
 > kubectl logs kubia-manual -c kubia

Kubernetes logs are in existence only as long as the pod still lives. If you
delete the pod, the logs associated with that pod is also deleted, so it is 
necessary for you to forward your logs to other tools like Splunk or 
set up centralized cluster-wide logging, which stores the logs into a 
central store. 


Port Forwarding
---------------
When you want to talk to a specific pod without going through a service, kubernetes
allows you to configure port forwarding to the pod

  > kubectl port-forward <pod-name> port1:port2

  will forward port1 of your machine to port2 of your pod
  to now in a new terminal, use curl localhost:port1 to hit the pods port2

  eg:
  > kubectl port-forward kubia-manual 8888:8080

  will forward your machines local port 8888 to port 8080 of the kubia-manual pod
  so now you can connect to your pod through the local port (in a different terminal since
  the port forwarder will open a port and not return til its closed)

  Allowing you to curl on the local port and hit the pod port:
  > curl localhost:8888
  You've hit kubia-manual
 

Labels - Organizing Pods
------------------------
Labels are a way of categorizing/tagging your pods.
 - groupings
 - team names
 - versions of apps (allowing multiple versions run in prod)
Labels allow you to organize your pods + other kubernetes objects.


Labels:
 - are arbitrary key value pairs that you assign to a resource
 - can be used to select for a subset of resources
 - resource can have many label 
 - the keys of the labels are unique within that resource
 - attach labels when you create resources, 
 - can be added/modified after the resource has been created, without the need to recreate the resource.
 - are defined in the meta data section of a resource spec

 see kubia-manual-with-labels.yaml for example of labels in a spec

Aside 
- a canary release is when you deploy a new version of an application next to the
  stable version, and only let a small fraction of users hit the canary version to see
  how it behaves before rolling it out to all users, preventing a large exposure if
  the release is unstable


Showing Labels/Selecting by Labels
----------------------------------
-labels will be visible to all person with access to that cluster
  Create the above kubia-manual-with-labels.yaml
  > kubectl create -f kubia-manual-with-labels.yaml

  Show the pod details with labels exposed
  > kubectl get pods --show-labels


-labels can also be used as selectors, allowing filtering by labels
 
  -L option will return ALL pods, displaying the labels provided as arguments to -L
    > kubectl get pods -L creation_method,env
      Will list all pods, and show all the creation method and env labels as columns

  -l option will filter and show only those pods that have the specified labels
    > kubectl get pods -l creation_method,env
      Will show only pods that have creation_method and env labels (but will not show the labels)

    > kubectl get pod -l env 
      Will show pods with the env label (regardless of value of env label)

    > kubectl get pod -l creation_method=manual,team=banking
      Will select those pods with creation_method=manual and team=banking
      But will not show those labels

   select pods that don't have a certain label, use single quotes and !
    > kubectl get pod -l '!env'
      Will select pods that do not have the env label

  -L -l will select by label, and display the labels
    > kubectl get pods -l team=banking -L team
      Will select pods labeled with team, and then list then showing the team values


Select pods based on multiple values, use in|notin operator
  > kubectl get pod -l creation_method in (prod,dev)
  > kubectl get pod -l creation_method notin (prod,dev)


Modifying Pods Labels
-------------------------
Add a label to an existing pod using the "label" command:
  > kubectl label pod kubia-manual creation_method=manual
    this will add creation_method label of kubia-manual to be manual

Overwrite an existing label by:
  > kubectl label pod kubia-manual-v2 env=debug --overwrite


Using Labels to control pod scheduling
--------------------------------------
In general you don't want to specify which nodes a pod should be created on, and just allow the kubernetes
cluster to allocate pods based on their resource requirements (eg, x memory, y cpu, z ssd/hdd)
but once in a while you will want to control which node a pod will be scheduled on based on certain 
characteristics of the node and the application requirement, eg deploy to nodes using ssd, 
or deploy on nodes using GPU, etc

Labels can be attached to any resource, not just pods.
So when you set up a node that is not homogenous with the rest of the cluster, you can label that
node differently. And like pods you can select any resouce based on some label.
 
For example this node is marked as being a gpu node
> kubectl label node gke2-aruc-548fea gpu=true

like wise you can select nodes and have them display the labels 
> kubectl get node -l gpu=true -L gpu
  will retrieve nodes where gpu is true, as well as display the gpu label


to specify a pod get deployed to a certain node you need to specify in your yaml manifest
a node selector criteria in the spec selection
  see kubia-with-node-selector.yaml

  apiVersion: v1
  kind: Pod
  metadata:
     name: kubia-gpu
  spec:
     nodeSelector: 
       gpu: "true"
     containers:
     - image: haja/kubia
       name: kubia
       ports:
       - containerPort: 8080
         protocol: TCP


Targeting pod deployment to specific node
-----------------------------------------
Each node has a unique label with the key kubernetes.io/hostname which is the actual hostname of the node.
You could potentially use this as a label selector to control your deployment, but in general doing so
would be bad practice because if such a node label does not exist, then the pod will become
unscheduled. 

Don't think in terms of individual nodes, but rather, node features you want to target


Pod Annotations
---------------
Pods and other objects can contain annotations. 
Annotations are key value pairs, similar to labels, but they are not meant to hold identifying information.
They cannot be used to group objects as there are no such thing as annotation selectors.

Annotations hold much more information then labels, and are mainly used by tools.
Some tools will add annotations, and some annotations get added by users.

Annotations are common used when a new feature gets added to kubernetes, before the API is clear
about the feature, as a way to document a feature, and then once the feature is agreed upon by
the community, new fields are added, and the annotation gets deprecated.

A good use of annotation is to add description to each pod or api object so that everyone using the
cluster can quickely get information about each object.

It is a good idea to use namespacing when annotating so that collisions doen't happen
   gecko.com/annotationkey=annotationvalue

An objects annotation will be in the resources metadata section.
To view the objects annotation, you need to objects yaml/json manifest
 > kubectl get pod <pod> -o yaml 

to add an annotation to an existing pod
 > kubectl annotate pod <pod-name> company.com/someannotaion="somevalue"

you can also view the added annotation by kubectl describe
 > kubectl describe pod <pod-name>


Kubernetes Namespaces
=====================

There will be times when you want to be able to split your objects into separate non-overlapping groups
so that you only perform operations inside one group at a time. This is where namespacing comes into play.
Namespacing provides a scope for object names.

Namespaces can also be used for multi-tenancy, or for separating environments eg qa, dev, e2e, etc.

So within different namespaces, objects can have the same name -- this may be useful.
For example in complex systems, you want to split your components into smaller distinct groups.
For example prod/pre-prod environments. So if you have a qal namespace and a prod namespace, the
objects name have the same name in each of those namespaces.

Nodes do not reside in a namespace, they are global and not tied to a namespace.

Kubectl commands for namespaces

-to get all the name spaces
 > kubectl get ns

you will see that there is a default namespace, a kube-public and kube-system namespace.

-to get objects within a certain namepace
> kubectl get pod --namespace kube-system

  will return all pods within the kube-system namepsace. It would make sense that objects used to 
run kubernetes will reside in their own namepsace, and don't get returned when you are running
your standard commands (which work for the default namespace), otherwise your the kube-system 
objects would clutter all your return values

Creating namespaces
===================
Namespaces are just another kubernetes resource, so they get created just like any other object, via
a manifest file:
 
   apiVersion: v1
   kind: Namespace
   metadata:
      name: custom-namespace

   > kubectl create -f custom-ns.yaml
     namespace "custom-namespace" created


You can also create namesapces with a command:
 > kubectl create namespace my-namespace

There are rules for object names (RFC 1035 - domain names) but namespaces may not conatin dots.

Creating resources in namespaces
=================================
When creatign objects, if you want to target a namespace, you can either declare the
namespace in the object manifest, or in the create command
> kubectl create -f my-manifest.yaml --namespace custom-namespace

When working objects in a namespace, if you don't specify the namespace, it will be the default
namespace, otherwise you need to specify it as part of your command.

Deleting objects
================
Likewise, if you want to delete objects in a certain namespace, you need to provide
the namespace as a parameter, other wise it assumes it is in default.

Switching namespaces
====================
You can switch between namespaces with the command
 > kubectl config set-context $(kubectl config current-context) --namespace <namespace to switch to>
 > kubectl config set-context $(kubectl config current-context) --namespace ass-kick-ns
    which switch to the namespace 'ass-kick-ns' and all kubectl commands will execute in that
    context, until you swtich back to default


Namespaces do not provide isolation of the running objects. 
Namespace only provides logical group distinction,
but objects can still communicate across namespaces if there is not ioslation.

Deleting Namespaces
===================
A namespace is just another resource, so you should be able to delete them
 > kubectl delete ns <name>

Deleting Pods
=============

To delete pods
> kubectl delete pod <pod name>, <pod name2>, ...

Kubernetes will send a SIGTERM signal to the process and wait for (30 secs) default time to allow the process
to gracefully shut down. If not, then it will send a SIGTERM signal

You can use the label selector to delete pods with that label
> kubectl delete pod -l creation_method=manual
 will delete all pods with that label and value.

You can delete an entire namespace, and all resources in the namespace automatically
> kubectl delete ns <namespace>

You delete all pods in a namespace
> kubectl delete pod --all

Deleting replication controllers
================================
anything that was created by a replication controller will attempt to scale up to its desired size
after a delete all command, so you have to delelete the replication controller as well
> kubectl delete all --all
 (this will delete services, rc, pods but will not delete secrets)

Or you can delete the replication controller manually

> kubectl get rc
  NAME     DESIRED      CURRENT     READY AGE
  kubia    1            1           1     6d

> kubectl delete rc kubia
  replicationcontroller "kubia" deleted
